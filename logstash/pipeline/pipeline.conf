input {
  beats {
      port => 5044
      host => "0.0.0.0"
      include_codec_tag => true
   }
}

filter {
  # The logtype has been set by Filebeat to distinct between OpenLog and TraceLog
  if ([logtype] == "openlog") {
    # Parse the received payload into a JSON-Object
    json {
       source => "[message]"
       target => "[message]"
    }
    # Use the given timestamp from OpenLog for the logstash timestamp send to ElasticSearch
    date {
       match => [ "timestamp", "UNIX_MS" ]
    }
    mutate {
      remove_field => ["host", "timestamp"]
    }
    # If a transactionElement event is received it must be re-arragended to support up-serts into ES
    if([transactionElement]) {
      ruby {
        code => "
          leg = event.get('transactionElement');
          no = event.get('[transactionElement][leg]');
          legNo = 'leg'.concat(no.to_s);
          event.set('[transactionElements][' + legNo + ']', leg);
        "
      }
      mutate {
       remove_field => ["transactionElement"]
      }
    }
    # If a transactionSummary event is received having a service context map it into a sinle object, as we can have only one
    if([transactionSummary][serviceContexts]) {
      ruby {
        code => "
          leg = event.get('[transactionSummary][serviceContexts][0]');
          event.set('[transactionSummary][serviceContext]', leg);
        "
      }
      mutate {
       remove_field => "[transactionSummary][serviceContexts]"
       rename => ["[transactionSummary][serviceContext][org]", "[transactionSummary][serviceContext][appOrg]" ]
      }
      # Future plan is to perform side-calls only if not yet already cached
      #mutate {
      #  add_field => { "apiCacheKey" => "%{[transactionSummary][serviceContext][service]}---%{[transactionSummary][path]}" }
      #}
      #memcached {
      #  namespace => "api_details"
      #  ttl => 300
      #  get => { "apiCacheKey" => "[transactionSummary][serviceContext][serviceOrg]" }
      #  remove_field => [ "apiCacheKey" ]
      #}
      #if ![transactionSummary][serviceContext][serviceOrg] {
      # Perform a side-call to load organization details for the API
      if([transactionSummary][serviceContext]) {
        http {
          url => "${API_BUILDER_URL}/api/elk/v1/api/lookup/api"
          query => {
            "apiName" => "%{[transactionSummary][serviceContext][service]}"
            "apiPath" => "%{[transactionSummary][path]}"
          }
          target_body => "apiDetails"
          add_field => { 
            "[transactionSummary][serviceContext][apiOrg]" => "%{[apiDetails][organizationName]}"
            "[transactionSummary][serviceContext][apiVersion]" => "%{[apiDetails][version]}" 
            "[transactionSummary][serviceContext][apiDeprecated]" => "%{[apiDetails][deprecated]}" 
            "[transactionSummary][serviceContext][apiState]" => "%{[apiDetails][state]}"
          }
          remove_field => [ "apiDetails", "headers" ]
        }
        # If the API-Lookup failed - Clone the event which is send to an Error index and shown in Traffic-Monitor
        if("_httprequestfailure" in [tags]) {
          clone {
            clones => ['errorEvent']
          }
          mutate { replace => { "[transactionElements][leg0][protocolInfo][http][status]" => "XXX" } }
          mutate { replace => { "[transactionElements][leg0][protocolInfo][http][statusText]" => "ERROR" } }
          mutate { replace => { "[transactionElements][leg0][protocolInfo][http][authSubjectId]" => "ID: %{[correlationId]}" } }
          mutate { replace => { "[transactionSummary][serviceContext][vhost]" => "Logstash Error" } }
          mutate { replace => { "[transactionSummary][serviceContext][method]" => "check the logs" } }
        }
      }
      #}
    }
  } else if([logtype] == "trace") {
    if [message] =~ /^#/ {
      drop { }
    }
    if [message] =~ /^\s*$/ {
      drop { }
    }
    grok {
      match => { "message" => "%{LOGLEVEL:level}%{SPACE}(?<loggedDate>\d{2}\/.{3}\/\d{4}:\d{2}:\d{2}:\d{2}\.\d{3})%{SPACE}\[%{WORD:fluff}\:%{WORD:correlationId}\]\s*%{GREEDYDATA:body}" }
    }
    date {
      # LoggedDate is provided like so 13/Jul/2020:15:26:35.108
      match => [ "loggedDate", "dd/MMM/yyyy:HH:mm:ss.SSS" ]
    }

    mutate {
      update => { "message" => "%{[body]}" } 
      remove_field => [ "host", "monthDay", "month", "year", "time", "fluff", "loggedDate", "body" ]
    }
  } else {
      mutate { add_field => { "correlationId" => "000000000000000000000000" } }
  }
}

output {
  if [correlationId] == "000000000000000000000000" {
    elasticsearch {
      hosts => "elasticsearch1:9200"
      index => "apigw-trace-messages-%{+YYYY.MM.dd}"
    }
  } else if [logtype] == "trace" {
    elasticsearch {
      hosts => "elasticsearch1:9200"
      index => "apigw-traffic-trace-%{+YYYY.MM.dd}"
    }
  } else if [type] == 'errorEvent' {
    elasticsearch {
      hosts => "elasticsearch1:9200"
      index => "apigw-traffic-details-err-%{+YYYY}"
      template => "${HOME}/config/traffic_details_index_template.json"
      template_overwrite => true
      document_id => "%{correlationId}"
      action => "update"
      doc_as_upsert => true
    }
  } else {
    elasticsearch {
      hosts => "elasticsearch1:9200"
      index => "apigw-traffic-details-%{+YYYY.MM.dd}"
      template => "${HOME}/config/traffic_details_index_template.json"
      template_overwrite => true
      document_id => "%{correlationId}"
      action => "update"
      doc_as_upsert => true
    }
  }
# Enable if you would like to see outgoing event messages
#  stdout {
#    codec => rubydebug
#  }
}
